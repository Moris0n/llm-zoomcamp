{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e73cff",
   "metadata": {},
   "source": [
    "# Homework Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfefdb3e-cd78-4734-b690-77cd81e55efd",
   "metadata": {},
   "source": [
    "\n",
    "### Q1. Runing Ollama with Docker\n",
    "```bash\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama \n",
    "\n",
    "docker exec -it ollama /bin/bash \n",
    "ollama --version\n",
    "```\n",
    "\n",
    "ollama version : ollama version is 0.1.48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d30225-fa7b-4f46-8ae3-b074a69ca54c",
   "metadata": {},
   "source": [
    "### Q2. Downloading an LLM\n",
    "\n",
    "```bash\n",
    "ollama pull gemma:2b\n",
    "```\n",
    "\n",
    "In docker, it saved the results into `/root/.ollama`\n",
    "\n",
    "We're interested in the metadata about this model. You can find\n",
    "it in `models/manifests/registry.ollama.ai/library`\n",
    "\n",
    "What's the content of the file related to gemma?\n",
    "\n",
    "```json\n",
    "{\"schemaVersion\":2,\n",
    "\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\n",
    "\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\n",
    "            \"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\n",
    "            \"size\":483},\n",
    "\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\n",
    "            \"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\n",
    "            \"size\":1678447520},\n",
    "        {\"mediaType\":\"application/vnd.ollama.image.license\",\n",
    "        \"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\n",
    "        \"size\":8433},\n",
    "        {\"mediaType\":\"application/vnd.ollama.image.template\",\n",
    "        \"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\n",
    "        \"size\":136},\n",
    "        {\"mediaType\":\"application/vnd.ollama.image.params\",\n",
    "        \"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\n",
    "        \"size\":84}]} \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9946a47-8260-48d6-b5a6-c9f5dafb78cc",
   "metadata": {},
   "source": [
    "### Q3. Running the LLM\n",
    "\n",
    "Test the following prompt: \"10 * 10\". What's the answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e71f1adc-bfdd-429c-8e9e-b6655f0c3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8444f021-f166-4071-95bb-dea3fb66e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gemma:2b',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21457a8a-4636-40ab-a18d-db47c2d5a7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is the answer:\n",
      "\n",
      "10 * 10 = 100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = llm('10 * 10')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf91692f-d313-446e-9e9b-89fb61796e4a",
   "metadata": {},
   "source": [
    "## Q4. Donwloading the weights \n",
    "\n",
    "We don't want to pull the weights every time we run\n",
    "a docker container. Let's do it once and have them available\n",
    "every time we start a container.\n",
    "\n",
    "First, we will need to change how we run the container.\n",
    "\n",
    "Instead of mapping the `/root/.ollama` folder to a named volume,\n",
    "let's map it to a local directory:\n",
    "\n",
    "```bash\n",
    "mkdir ollama_files\n",
    "\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ./ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "Now pull the model:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama pull gemma:2b \n",
    "```\n",
    "\n",
    "What's the size of the `ollama_files/models` folder? \n",
    "\n",
    "``` bash\n",
    "du -sh ollama_files/models/\n",
    "1.6G    ollama_files/models/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e658f-3dc0-4b19-b315-85b60ff9bc5e",
   "metadata": {},
   "source": [
    "### Q5. Adding the weights\n",
    "```bash\n",
    "# Use the base Ollama image from Docker Hub\n",
    "FROM ollama/ollama\n",
    "\n",
    "# Copy model weights from local directory to Docker image\n",
    "COPY ./ollama_files/models /root/.ollama/models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63792e-3bb2-494b-9459-b1640ea87f64",
   "metadata": {},
   "source": [
    "### Q6. Serving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb0d0d28-5f20-4a67-8ad1-5ac8b798f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gemma:2b',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bda66fb-0ea5-4495-bbe3-07f1993ee6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's the formula for energy:\n",
      "\n",
      "**E = K + U**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **E** is the energy in joules (J)\n",
      "* **K** is the kinetic energy in joules (J)\n",
      "* **U** is the potential energy in joules (J)\n",
      "\n",
      "**Kinetic energy (K)** is the energy an object possesses when it moves or is in motion. It is calculated as half the product of an object's mass (m) and its velocity (v) squared:\n",
      "\n",
      "**K = 1/2mv^2**\n",
      "\n",
      "**Potential energy (U)** is the energy an object possesses due to its position or configuration. It is calculated as the product of an object's mass, gravitational constant (g), and height or position above a reference point.\n",
      "\n",
      "**U = mgh**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **m** is the mass in kilograms (kg)\n",
      "* **g** is the gravitational constant (9.8 m/s^2)\n",
      "* **h** is the height or position in meters (m)\n",
      "\n",
      "The formula shows that energy can be expressed as the sum of kinetic and potential energy. The kinetic energy is a measure of the object's ability to do work, while the potential energy is a measure of the object's ability to do work against a force.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What's the formula for energy?\"\n",
    "\n",
    "res = llm(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "095a1460-2ac9-4260-9253-3b42c0dc410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 261\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "# Tokenize the results\n",
    "tokens = encoding.encode(res)\n",
    "\n",
    "# Calculate the number of tokens\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c976ea1-230f-4c89-9172-dc9cfd318eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
