{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef4c888-c082-4251-91c6-493ed393469b",
   "metadata": {},
   "source": [
    "# Homework: Evaluation and Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72bae45-8fc4-403b-b68b-286eaaa7569c",
   "metadata": {},
   "source": [
    "### Getting the ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4146cc-88ba-4980-9c9e-b7e2aca8c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e281c11-7540-4c54-82c8-eb817115a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv'\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)\n",
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445292fc-3bb8-490b-9092-6b7deea67607",
   "metadata": {},
   "source": [
    "### Q1. Getting the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f629fa-2a57-4bdb-8f14-82a31f3e9c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8ec9fd7-0954-4d81-90c7-b3cbea0f8a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer_llm     You can sign up for the course by visiting the...\n",
       "answer_orig    Machine Learning Zoomcamp FAQ\\nThe purpose of ...\n",
       "document                                                0227b872\n",
       "question                     Where can I sign up for the course?\n",
       "course                                 machine-learning-zoomcamp\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6307fa3c-cc54-468c-9ee8-d9b878a186ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_llm = df.iloc[0].answer_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91468cf-72fa-484b-bc91-295979a267e9",
   "metadata": {},
   "source": [
    "What's the first value of the resulting vector?  \n",
    "  \n",
    "**-0.42**  \n",
    "-0.22  \n",
    "-0.02  \n",
    "0.21  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3416ae3e-8ef1-431f-a716-0e0ff532ff90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42244655"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_llm_vector = embedding_model.encode(answer_llm)\n",
    "answer_llm_vector[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55aa36-d683-4903-bd27-3c725768e269",
   "metadata": {},
   "source": [
    "## Q2. Computing the dot product\n",
    "\n",
    "\n",
    "Now for each answer pair, let's create embeddings and compute dot product between them\n",
    "\n",
    "We will put the results (scores) into the `evaluations` list\n",
    "\n",
    "What's the 75% percentile of the score?\n",
    "\n",
    "* 21.67\n",
    "* **31.67**\n",
    "* 41.67\n",
    "* 51.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cdf6fa5-5bdb-470d-8344-629577e80430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = embedding_model.encode(answer_llm)\n",
    "    v_orig = embedding_model.encode(answer_orig)\n",
    "    \n",
    "    return v_llm.dot(v_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a02fc769-83d1-4491-8114-2f3d664eeb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gpt4o = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf43ae2-cfff-470f-ba4c-68d4cc3b131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [02:22<00:00,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for record in tqdm(results_gpt4o):\n",
    "    sim = compute_similarity(record)\n",
    "    evaluations.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83f42d25-d721-408f-bd86-cde326b86287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean      27.495996\n",
       "std        6.384742\n",
       "min        4.547924\n",
       "25%       24.307844\n",
       "50%       28.336870\n",
       "75%       31.674309\n",
       "max       39.476013\n",
       "Name: similarity, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['similarity'] = evaluations\n",
    "df['similarity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40348ac7-9ed4-4cb7-a65c-4824e4f24050",
   "metadata": {},
   "source": [
    "## Q3. Computing the cosine\n",
    "\n",
    "From Q2, we can see that the results are not within the [0, 1] range. It's because the vectors coming from this model are not normalized.\n",
    "\n",
    "So we need to normalize them.\n",
    "\n",
    "To do it, we \n",
    "\n",
    "* Compute the norm of a vector\n",
    "* Divide each element by this norm\n",
    "\n",
    "So, for vector `v`, it'll be `v / ||v||`\n",
    "\n",
    "In numpy, this is how you do it:\n",
    "\n",
    "```python\n",
    "norm = np.sqrt((v * v).sum())\n",
    "v_norm = v / norm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b9d9fae-26a6-447d-9bdf-bdea86c1144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def norm_vector(v) :\n",
    "    norm = np.sqrt((v * v).sum())\n",
    "    v_norm = v / norm\n",
    "    return v_norm\n",
    "\n",
    "def compute_cos_similarity(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = embedding_model.encode(answer_llm)\n",
    "    v_orig = embedding_model.encode(answer_orig)\n",
    "    \n",
    "    return norm_vector(v_llm).dot(norm_vector(v_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df4f184a-609d-42e4-b332-a3a04b8c1898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [02:21<00:00,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(results_gpt4o):\n",
    "    sim = compute_cos_similarity(record)\n",
    "    evaluations.append(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845cce6-fd70-415f-af63-2c9cbae5f6a7",
   "metadata": {},
   "source": [
    "What's the 75% cosine in the scores?\n",
    "\n",
    "* 0.63\n",
    "* 0.73\n",
    "* **0.83**\n",
    "* 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eeba5b9-ee4b-49a3-9e2b-9f25e2e3996e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean       0.728393\n",
       "std        0.157755\n",
       "min        0.125357\n",
       "25%        0.651273\n",
       "50%        0.763761\n",
       "75%        0.836235\n",
       "max        0.958796\n",
       "Name: cosine, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cosine'] = evaluations\n",
    "df['cosine'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb16c70-65d7-438c-bdcf-cede4aa580ae",
   "metadata": {},
   "source": [
    "## Q4. Rouge\n",
    "\n",
    "the ROUGE score - This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (`doc_id=5170565b`)\n",
    "\n",
    "```\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "```\n",
    "\n",
    "There are three scores: `rouge-1`, `rouge-2` and `rouge-l`, and precision, recall and F1 score for each.\n",
    "\n",
    "* `rouge-1` - the overlap of unigrams,\n",
    "* `rouge-2` - bigrams,\n",
    "* `rouge-l` - the longest common subsequence\n",
    "\n",
    "What's the F score for `rouge-1`?\n",
    "\n",
    "- 0.35\n",
    "- **0.45**\n",
    "- 0.55\n",
    "- 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "548f9379-85ae-4ec0-a1e3-788a31dfce2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer_llm     Yes, all sessions are recorded, so if you miss...\n",
       "answer_orig    Everything is recorded, so you won’t miss anyt...\n",
       "document                                                5170565b\n",
       "question                    Are sessions recorded if I miss one?\n",
       "course                                 machine-learning-zoomcamp\n",
       "similarity                                             32.344711\n",
       "cosine                                                  0.777956\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a15a60c9-f379-4190-8674-3b0ad605043f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.45454545454545453,\n",
       "   'p': 0.45454545454545453,\n",
       "   'f': 0.45454544954545456},\n",
       "  'rouge-2': {'r': 0.21621621621621623,\n",
       "   'p': 0.21621621621621623,\n",
       "   'f': 0.21621621121621637},\n",
       "  'rouge-l': {'r': 0.3939393939393939,\n",
       "   'p': 0.3939393939393939,\n",
       "   'f': 0.393939388939394}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "r = df.iloc[10]\n",
    "\n",
    "rouge_scorer = Rouge()\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04476ae6-4648-466d-8a79-41105386e3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454544954545456"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0]['rouge-1']['f']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0b512-eca1-474b-b392-41ffe936a663",
   "metadata": {},
   "source": [
    "## Q5. Average rouge score\n",
    "\n",
    "Let's compute the average F-score between `rouge-1`, `rouge-2` and `rouge-l` for the same record from Q4\n",
    "\n",
    "- **0.35**\n",
    "- 0.45\n",
    "- 0.55\n",
    "- 0.65\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bb97527-564b-4dd1-8474-f5a8a2b23010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>p</th>\n",
       "      <th>f</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge-1</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-2</th>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge-l</th>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.393939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   r         p         f\n",
       "rouge_type                              \n",
       "rouge-1     0.454545  0.454545  0.454545\n",
       "rouge-2     0.216216  0.216216  0.216216\n",
       "rouge-l     0.393939  0.393939  0.393939"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize lists to store the values\n",
    "rouge_types = []\n",
    "recall = []\n",
    "precision = []\n",
    "f_score = []\n",
    "\n",
    "# Loop through the data to extract the values\n",
    "for rouge_type, metrics in scores[0].items():\n",
    "    rouge_types.append(rouge_type)\n",
    "    recall.append(metrics['r'])\n",
    "    precision.append(metrics['p'])\n",
    "    f_score.append(metrics['f'])\n",
    "    \n",
    "# Create a DataFrame\n",
    "df_scores = pd.DataFrame({\n",
    "    'rouge_type': rouge_types,\n",
    "    'r': recall,\n",
    "    'p': precision,\n",
    "    'f': f_score\n",
    "})\n",
    "\n",
    "df_scores.set_index('rouge_type', inplace=True)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e75eaa0-fc9d-405d-bb52-5b7100d45dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35490034990035496"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores.f.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68846265-e57c-47b0-ac02-81ec018a1cfd",
   "metadata": {},
   "source": [
    "## Q6. Average rouge score for all the data points\n",
    "\n",
    "Now let's compute the score for all the records and create a dataframe from them.\n",
    "\n",
    "What's the average `rouge_2` across all the records?\n",
    "\n",
    "- 0.10\n",
    "- **0.20**\n",
    "- 0.30\n",
    "- 0.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0205100-2380-45f2-b1d7-8c0278cece46",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    scores = rouge_scorer.get_scores(row['answer_llm'], row['answer_orig'])\n",
    "    scores_list.append(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30a30459-4d11-4d6d-a0c9-998ac5b5fe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31042/252543045.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_rouge = pd.concat([df_rouge, pd.DataFrame(data)], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>rouge-avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.072882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.091435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.389610</td>\n",
       "      <td>0.327658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.150821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.142076</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.120219</td>\n",
       "      <td>0.098731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.604570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.460432</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.535991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.654867</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>0.618851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.304762</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>0.247252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.118954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rouge-1   rouge-2   rouge-l  rouge-avg\n",
       "0    0.095238  0.028169  0.095238   0.072882\n",
       "1    0.125000  0.055556  0.093750   0.091435\n",
       "2    0.415584  0.177778  0.389610   0.327658\n",
       "3    0.216216  0.047059  0.189189   0.150821\n",
       "4    0.142076  0.033898  0.120219   0.098731\n",
       "..        ...       ...       ...        ...\n",
       "295  0.654545  0.540984  0.618182   0.604570\n",
       "296  0.590164  0.460432  0.557377   0.535991\n",
       "297  0.654867  0.564516  0.637168   0.618851\n",
       "298  0.304762  0.132231  0.304762   0.247252\n",
       "299  0.179487  0.023529  0.153846   0.118954\n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rouge = pd.DataFrame(columns=['rouge-1','rouge-2','rouge-l','rouge-avg'])\n",
    "data = []\n",
    "\n",
    "for i in range(len(scores_list)):\n",
    "    rouge_1 = scores_list[i]['rouge-1']['f']\n",
    "    rouge_2 = scores_list[i]['rouge-2']['f']\n",
    "    rouge_l = scores_list[i]['rouge-l']['f']\n",
    "    rouge_avg = (rouge_1 + rouge_2 + rouge_l) / 3\n",
    "    data.append({\n",
    "        'rouge-1': rouge_1,\n",
    "        'rouge-2': rouge_2,\n",
    "        'rouge-l': rouge_l,\n",
    "        'rouge-avg': rouge_avg\n",
    "    })\n",
    "df_rouge = pd.concat([df_rouge, pd.DataFrame(data)], ignore_index=True)\n",
    "df_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f303318-e26a-40fd-9bd5-e06f5a730ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20696501983423318"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rouge['rouge-2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c240b30-3c94-4049-90b7-296145f4ad2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
